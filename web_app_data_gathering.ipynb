{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28116775",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "def fetch_monthly_hourly(symbol, year, month):\n",
    "    start = pd.Timestamp(f\"{year}-{month:02d}-01\")\n",
    "    end = (start + pd.offsets.MonthBegin(1))\n",
    "\n",
    "    start_ts = int(start.timestamp() * 1000)\n",
    "    end_ts = int(end.timestamp() * 1000)\n",
    "\n",
    "    all_data = []\n",
    "    while start_ts < end_ts:\n",
    "        params = {\n",
    "            \"symbol\": symbol,\n",
    "            \"interval\": \"1h\",\n",
    "            \"startTime\": start_ts,\n",
    "            \"limit\": 1000\n",
    "        }\n",
    "        response = requests.get(\"https://api.binance.com/api/v3/klines\", params=params)\n",
    "        data = response.json()\n",
    "        if not data:\n",
    "            break\n",
    "\n",
    "        all_data.extend(data)\n",
    "        start_ts = data[-1][0] + 1\n",
    "        time.sleep(0.25)\n",
    "\n",
    "    df = pd.DataFrame(all_data, columns=[\"timestamp\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"_close_time\", \"_quote_asset_volume\", \"_num_trades\", \"_taker_buy_base_vol\", \"_taker_buy_quote_vol\", \"_ignore\"])\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], unit=\"ms\")\n",
    "    df = df[[\"timestamp\", \"open\", \"high\", \"low\", \"close\", \"volume\"]]\n",
    "    df[[\"open\", \"high\", \"low\", \"close\", \"volume\"]] = df[[\"open\", \"high\", \"low\", \"close\", \"volume\"]].astype(float)\n",
    "    return df\n",
    "\n",
    "\n",
    "def getDataForPeriod(start, end, ticker):\n",
    "    all_dfs = []\n",
    "\n",
    "    for year in [start, end]:\n",
    "        start_month = 1\n",
    "        end_month = 12\n",
    "\n",
    "        if year == 2024:\n",
    "            start_month = 1\n",
    "        if year == 2025:\n",
    "            end_month = 5\n",
    "\n",
    "        for month in range(start_month, end_month + 1):\n",
    "            print(f\"Fetching {ticker} {year}-{month:02d}\")\n",
    "            df = fetch_monthly_hourly(ticker, year, month)\n",
    "            all_dfs.append(df)\n",
    "\n",
    "    return pd.concat(all_dfs)\n",
    "\n",
    "\n",
    "def resample(df, interval=\"4h\", column_map=None):\n",
    "    df.set_index(\"timestamp\", inplace=True)\n",
    "    if column_map is None:\n",
    "        column_map = {\n",
    "            \"open\": \"first\",\n",
    "            \"high\": \"max\",\n",
    "            \"low\": \"min\",\n",
    "            \"close\": \"last\",\n",
    "            \"volume\": \"sum\"\n",
    "        }\n",
    "    resampled = df.resample(interval).agg(column_map).dropna()\n",
    "    resampled.reset_index(inplace=True)\n",
    "    df.reset_index(inplace=True)\n",
    "    return resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d7e4ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching BTCUSDT 2024-01\n",
      "Fetching BTCUSDT 2024-02\n",
      "Fetching BTCUSDT 2024-03\n",
      "Fetching BTCUSDT 2024-04\n",
      "Fetching BTCUSDT 2024-05\n",
      "Fetching BTCUSDT 2024-06\n",
      "Fetching BTCUSDT 2024-07\n",
      "Fetching BTCUSDT 2024-08\n",
      "Fetching BTCUSDT 2024-09\n",
      "Fetching BTCUSDT 2024-10\n",
      "Fetching BTCUSDT 2024-11\n",
      "Fetching BTCUSDT 2024-12\n",
      "Fetching BTCUSDT 2025-01\n",
      "Fetching BTCUSDT 2025-02\n",
      "Fetching BTCUSDT 2025-03\n",
      "Fetching BTCUSDT 2025-04\n",
      "Fetching BTCUSDT 2025-05\n"
     ]
    }
   ],
   "source": [
    "btc_1h = getDataForPeriod(2024, 2025, 'BTCUSDT')\n",
    "btc_1h.drop_duplicates(inplace=True)\n",
    "btc_4h = resample(btc_1h, interval=\"4h\")\n",
    "btc_1d = resample(btc_1h, interval=\"1d\")\n",
    "btc_1w = resample(btc_1h, interval=\"1W\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59533814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching ETHUSDT 2024-01\n",
      "Fetching ETHUSDT 2024-02\n",
      "Fetching ETHUSDT 2024-03\n",
      "Fetching ETHUSDT 2024-04\n",
      "Fetching ETHUSDT 2024-05\n",
      "Fetching ETHUSDT 2024-06\n",
      "Fetching ETHUSDT 2024-07\n",
      "Fetching ETHUSDT 2024-08\n",
      "Fetching ETHUSDT 2024-09\n",
      "Fetching ETHUSDT 2024-10\n",
      "Fetching ETHUSDT 2024-11\n",
      "Fetching ETHUSDT 2024-12\n",
      "Fetching ETHUSDT 2025-01\n",
      "Fetching ETHUSDT 2025-02\n",
      "Fetching ETHUSDT 2025-03\n",
      "Fetching ETHUSDT 2025-04\n",
      "Fetching ETHUSDT 2025-05\n"
     ]
    }
   ],
   "source": [
    "eth_1h = getDataForPeriod(2024, 2025, 'ETHUSDT')\n",
    "eth_1h.drop_duplicates(inplace=True)\n",
    "eth_4h = resample(eth_1h, interval=\"4h\")\n",
    "eth_1d = resample(eth_1h, interval=\"1d\")\n",
    "eth_1w = resample(eth_1h, interval=\"1W\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08843034",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Helper function that fetches the data.\n",
    "def getStockData(API_KEY, symbol, function, market=None, outputsize=\"full\", interval=None, month=None, count=0):\n",
    "    symbolString = \"\"\n",
    "    intervalString = \"\"\n",
    "    outputsizeString = \"\"\n",
    "    monthString = \"\"\n",
    "    marketString = \"\"\n",
    "\n",
    "    count = checkRequests(count)\n",
    "    \n",
    "    if symbol != None:\n",
    "        symbolString = f\"&symbol={symbol}\" \n",
    "    if interval != None:\n",
    "        intervalString = f\"&interval={interval}\" \n",
    "    if outputsize != None:\n",
    "        outputsizeString = f\"&outputsize={outputsize}\" \n",
    "    if month != None:\n",
    "        monthString = f\"&month={month}\"\n",
    "    if market != None:\n",
    "        marketString = f\"&market={market}\"\n",
    "\n",
    "    URL = f\"https://www.alphavantage.co/query?function={function}{symbolString}{marketString}{intervalString}{outputsizeString}{monthString}&apikey={API_KEY}\"\n",
    "    response = requests.get(URL)\n",
    "    data = response.json()\n",
    "    if data == None:\n",
    "        print(\"None\")\n",
    "    data = data.get(f\"Time Series ({interval})\", {})\n",
    "    df = pd.DataFrame.from_dict(data, orient=\"index\")\n",
    "    df.insert(0, \"timestamp\", df.index)\n",
    "    return df, count\n",
    "\n",
    "# Gets stock data for a a period of years specified by starting year and ending year.\n",
    "def getDataForPeriod(API_KEY, symbol, function, interval, yearStart, yearEnd, count, market=None):\n",
    "    dfList = []\n",
    "    for year in range(yearStart, yearEnd+1):\n",
    "        print(f\"Getting {symbol} data for {year}\")\n",
    "        for month in range(1, 13):\n",
    "            month = f\"{year}-{str(month).zfill(2)}\"\n",
    "            df, count = getStockData(API_KEY=API_KEY, symbol=symbol, function=function, interval=interval, month=month, count=count, market=market)\n",
    "            dfList.append(df)\n",
    "    full_df = pd.concat(dfList, ignore_index=True)\n",
    "    return full_df, count\n",
    "\n",
    "# Checks if 75 requests has been made and sets a 1 minute timer before requesting again.\n",
    "def checkRequests(count):\n",
    "    if count == 75:\n",
    "        print(\"API requests reached for min... Waiting\")\n",
    "        time.sleep(62)\n",
    "        count = 0\n",
    "    count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37176bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(df):\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "    df = df.sort_values(\"timestamp\").set_index(\"timestamp\", drop=False)\n",
    "    cols = [\"1. open\", \"2. high\", \"3. low\", \"4. close\", \"5. volume\"]\n",
    "    df[cols] = df[cols].apply(pd.to_numeric)\n",
    "    rename_map = {\"1. open\": \"open\",                                            # Map for renaming columns to standard\n",
    "                  \"2. high\": \"high\", \n",
    "                  \"3. low\": \"low\",\n",
    "                  \"4. close\": \"close\", \n",
    "                  \"5. volume\": \"volume\", \n",
    "                  \"date\": \"timestamp\",\n",
    "                  \"volume usdt\": \"volume\",\n",
    "                  }\n",
    "    \n",
    "    df.rename(columns=rename_map, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "521b21c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting AAPL data for 2024\n",
      "Getting AAPL data for 2025\n",
      "Getting TSLA data for 2024\n",
      "Getting TSLA data for 2025\n",
      "Getting AMZN data for 2024\n",
      "Getting AMZN data for 2025\n"
     ]
    }
   ],
   "source": [
    "API_KEY = \"5HUC90FRQ4H9PK0Q\"\n",
    "\n",
    "count = 0\n",
    "\n",
    "aapl_1h, count = getDataForPeriod(API_KEY=API_KEY, symbol=\"AAPL\", function=\"TIME_SERIES_INTRADAY\", interval=\"60min\", yearStart=2024, yearEnd=2025, count=count)\n",
    "aapl_1h.drop_duplicates(inplace=True)\n",
    "aapl_1h = prepare(aapl_1h)\n",
    "aapl_4h = resample(aapl_1h, interval=\"4h\")\n",
    "aapl_1d = resample(aapl_1h, interval=\"1d\")\n",
    "aapl_1w = resample(aapl_1h, interval=\"1W\")\n",
    "\n",
    "tsla_1h, count = getDataForPeriod(API_KEY=API_KEY, symbol=\"TSLA\", function=\"TIME_SERIES_INTRADAY\", interval=\"60min\", yearStart=2024, yearEnd=2025, count=count)\n",
    "tsla_1h.drop_duplicates(inplace=True)\n",
    "tsla_1h = prepare(tsla_1h)\n",
    "tsla_4h = resample(tsla_1h, interval=\"4h\")\n",
    "tsla_1d = resample(tsla_1h, interval=\"1d\")\n",
    "tsla_1w = resample(tsla_1h, interval=\"1W\")\n",
    "\n",
    "amzn_1h, count = getDataForPeriod(API_KEY=API_KEY, symbol=\"AMZN\", function=\"TIME_SERIES_INTRADAY\", interval=\"60min\", yearStart=2024, yearEnd=2025, count=count)\n",
    "amzn_1h.drop_duplicates(inplace=True)\n",
    "amzn_1h = prepare(amzn_1h)\n",
    "amzn_4h = resample(amzn_1h, interval=\"4h\")\n",
    "amzn_1d = resample(amzn_1h, interval=\"1d\")\n",
    "amzn_1w = resample(amzn_1h, interval=\"1W\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2dcc9453",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNewsData(API_KEY, tickers, time_from, time_to, count=0):\n",
    "    tickerString = \"\"\n",
    "    topicString = \"\"\n",
    "    fromString = \"\"\n",
    "    toString = \"\"\n",
    "\n",
    "    count = checkRequests(count)\n",
    "\n",
    "    if tickers != None:\n",
    "        tickerString = f\"&tickers={tickers}\"\n",
    "    if time_from != None:\n",
    "        fromString = f\"&time_from={time_from}\"\n",
    "    if time_to != None:\n",
    "        toString = f\"&time_to={time_to}\"\n",
    "    \n",
    "    URL = f\"https://www.alphavantage.co/query?function=NEWS_SENTIMENT{tickerString}{fromString}{toString}&apikey={API_KEY}&limit=1000&sort=EARLIEST\"\n",
    "    response = requests.get(URL)\n",
    "    data = response.json()\n",
    "    data = data['feed']\n",
    "    df = pd.DataFrame.from_dict(data)\n",
    "    return df, count\n",
    "\n",
    "def checkRequests(count):\n",
    "    if count == 75:\n",
    "        print(\"API requests reached for min... Waiting\")\n",
    "        time.sleep(62)\n",
    "        count = 0\n",
    "    count += 1\n",
    "    return count\n",
    "\n",
    "def getNewsForPeriod(API_KEY, tickers, dateStart, dateEnd, count):\n",
    "    start = datetime.strptime(dateStart, '%Y%m%d')\n",
    "    end = datetime.strptime(dateEnd, '%Y%m%d')\n",
    "    dfList = []\n",
    "\n",
    "    current = start\n",
    "    print(f\"Getting data for {tickers} from {start.date()} to {end.date()}\")\n",
    "    while current < end:\n",
    "        time_from = current.strftime('%Y%m%dT0001')\n",
    "        if (current + relativedelta(weeks=3)) > end:\n",
    "            current = end\n",
    "        else:\n",
    "            current += relativedelta(weeks=3)\n",
    "        time_to = current.strftime('%Y%m%dT2359')\n",
    "        df, count = getNewsData(API_KEY=API_KEY, tickers=tickers, time_from=time_from, time_to=time_to, count=count)\n",
    "        dfList.append(df)\n",
    "    full_df = pd.concat(dfList, ignore_index=True)\n",
    "    return full_df, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4004e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting data for AAPL from 2024-01-01 to 2025-06-01\n",
      "API requests reached for min... Waiting\n",
      "Getting data for TSLA from 2024-01-01 to 2025-06-01\n",
      "Getting data for AMZN from 2024-01-01 to 2025-06-01\n",
      "Getting data for CRYPTO:BTC from 2024-01-01 to 2025-06-01\n",
      "API requests reached for min... Waiting\n",
      "Getting data for CRYPTO:ETH from 2024-01-01 to 2025-06-01\n"
     ]
    }
   ],
   "source": [
    "API_KEY = \"5HUC90FRQ4H9PK0Q\"\n",
    "\n",
    "aapl_news, count = getNewsForPeriod(API_KEY=API_KEY, tickers=\"AAPL\", dateStart=\"20240101\", dateEnd=\"20250601\", count=count)\n",
    "tsla_news, count = getNewsForPeriod(API_KEY=API_KEY, tickers=\"TSLA\", dateStart=\"20240101\", dateEnd=\"20250601\", count=count)\n",
    "amzn_news, count = getNewsForPeriod(API_KEY=API_KEY, tickers=\"AMZN\", dateStart=\"20240101\", dateEnd=\"20250601\", count=count)\n",
    "btc_news, count = getNewsForPeriod(API_KEY=API_KEY, tickers=\"CRYPTO:BTC\", dateStart=\"20240101\", dateEnd=\"20250601\", count=count)\n",
    "eth_news, count = getNewsForPeriod(API_KEY=API_KEY, tickers=\"CRYPTO:ETH\", dateStart=\"20240101\", dateEnd=\"20250601\", count=count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56ead694",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import re\n",
    "\n",
    "all_topics = []\n",
    "\n",
    "def clean_data(df, tickerName=''):\n",
    "    copy_df = df.copy()\n",
    "    copy_df = copy_df[['title', 'time_published', 'summary', 'topics', 'overall_sentiment_score', 'ticker_sentiment']]\n",
    "    \n",
    "    for topic_list in copy_df['topics']:\n",
    "        if isinstance(topic_list, str):\n",
    "            topic_list = ast.literal_eval(topic_list)\n",
    "        for topic_dict in topic_list:\n",
    "            if topic_dict['topic'] not in all_topics:\n",
    "                all_topics.append(topic_dict['topic'])\n",
    "\n",
    "    copy_df[['ticker_relevance', 'ticker_sentiment']] = copy_df['ticker_sentiment'].apply(lambda row: add_ticker_info(row, tickerName))\n",
    "    weighted_df = copy_df['topics'].apply(add_topic_relevance)\n",
    "    weighted_df = weighted_df.rename(columns=lambda x: f\"topic_{x}\")\n",
    "    copy_df = pd.concat([copy_df, weighted_df], axis = 1)\n",
    "    copy_df = copy_df.drop(columns=['topics'])\n",
    "\n",
    "    copy_df['time_published'] = pd.to_datetime(copy_df['time_published'], format='%Y%m%dT%H%M%S')\n",
    "    copy_df['time_published'] = copy_df['time_published'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    copy_df = copy_df.rename(columns={'time_published': 'timestamp'})\n",
    "    copy_df = copy_df.drop_duplicates()\n",
    "    copy_df = copy_df.dropna()\n",
    "    copy_df['title'] = copy_df['title'].apply(clean_text)\n",
    "    copy_df['summary'] = copy_df['summary'].apply(clean_text)\n",
    "    copy_df = copy_df.set_index('timestamp')\n",
    "    return copy_df\n",
    "\n",
    "def add_topic_relevance(topics_row):\n",
    "    if isinstance(topics_row, str):\n",
    "        topics_list = ast.literal_eval(topics_row)\n",
    "    else:\n",
    "        topics_list = topics_row\n",
    "\n",
    "    topic_weights = {topic: 0.0 for topic in all_topics}\n",
    "\n",
    "    for topic_info in topics_list:\n",
    "        topic_name = topic_info['topic']\n",
    "        relevance = float(topic_info['relevance_score'])\n",
    "        if topic_name in topic_weights:\n",
    "            topic_weights[topic_name] = relevance\n",
    "\n",
    "    return pd.Series(topic_weights)\n",
    "\n",
    "\n",
    "def add_ticker_info(ticker_row, tickerName):\n",
    "    if isinstance(ticker_row, str):\n",
    "        ticker_list = ast.literal_eval(ticker_row)\n",
    "    else:\n",
    "        ticker_list = ticker_row\n",
    "\n",
    "    for ticker_info in ticker_list:\n",
    "        if ticker_info['ticker'] == tickerName:\n",
    "            relevance = float(ticker_info['relevance_score'])\n",
    "            sentiment = float(ticker_info['ticker_sentiment_score'])\n",
    "            return pd.Series([relevance, sentiment])\n",
    "    \n",
    "    return pd.Series([0.0, 0.0])\n",
    "\n",
    "def clean_text(text):\n",
    "    if pd.isnull(text):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46868ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl_news = clean_data(aapl_news, 'AAPL')\n",
    "tsla_news = clean_data(tsla_news, 'TSLA')\n",
    "amzn_news = clean_data(amzn_news, 'AMZN')\n",
    "btc_news = clean_data(btc_news, 'CRYPTO:BTC')\n",
    "eth_news = clean_data(eth_news, 'CRYPTO:ETH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f214a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ta, ta.trend, ta.momentum\n",
    "\n",
    "def add_ta_features(df, timeframe,rsi_window=14, sma_windows=(5, 20)):\n",
    "    df = df.copy()\n",
    "    \n",
    "    rsi = ta.momentum.RSIIndicator(close=df[\"close\"], window=rsi_window)\n",
    "    df[\"rsi_14\"] = rsi.rsi()\n",
    "    \n",
    "    macd = ta.trend.MACD(close=df[\"close\"])\n",
    "    df[\"macd\"] = macd.macd()\n",
    "    df[\"macd_signal\"] = macd.macd_signal()\n",
    "\n",
    "    sma_short = ta.trend.SMAIndicator(close=df[\"close\"], window=sma_windows[0])\n",
    "    sma_long = ta.trend.SMAIndicator(close=df[\"close\"], window=sma_windows[1])\n",
    "    \n",
    "    df[f\"sma_{sma_windows[0]}\"] = sma_short.sma_indicator()\n",
    "    df[f\"sma_{sma_windows[1]}\"] = sma_long.sma_indicator()\n",
    "\n",
    "    df[\"return\"] = df[\"close\"].pct_change(periods=timeframe)\n",
    "\n",
    "    df[\"future_return\"] = df[\"close\"].shift(-1) / df[\"close\"] - 1\n",
    "    df[\"target\"] = (df[\"future_return\"] > 0).astype(int)\n",
    "    df = df.dropna()\n",
    "    return df\n",
    "\n",
    "def aggregate_and_forwardfill_news(news_df, timeframe='1H'):\n",
    "    news_df = news_df.reset_index()\n",
    "    news_df['timestamp'] = pd.to_datetime(news_df['timestamp'])\n",
    "    # news_df.set_index(\"timestamp\", inplace=True)\n",
    "    numeric_cols = news_df.select_dtypes(include='number').columns.tolist()\n",
    "    numeric_cols.append('timestamp')\n",
    "    news_df = news_df[numeric_cols]\n",
    "    news_df = news_df.set_index('timestamp')\n",
    "    news_aggregated = news_df.resample(timeframe).mean()\n",
    "    sentiment_columns = news_aggregated.columns\n",
    "    news_aggregated[sentiment_columns] = news_aggregated[sentiment_columns].ffill()\n",
    "    news_aggregated[sentiment_columns] = news_aggregated[sentiment_columns].fillna(0)\n",
    "    news_aggregated = news_aggregated.reset_index()\n",
    "    # news_df.reset_index(inplace=True)\n",
    "    return news_aggregated\n",
    "\n",
    "def merge_df(df1, df2):\n",
    "    df1['timestamp'] = pd.to_datetime(df1['timestamp'])\n",
    "    df2['timestamp'] = pd.to_datetime(df2['timestamp'])\n",
    "\n",
    "    df1 = df1.set_index('timestamp')\n",
    "    df2 = df2.set_index('timestamp')\n",
    "\n",
    "    merged_df = df1.join(df2, how='left')\n",
    "    merged_df = merged_df.reset_index()\n",
    "    sentiment_cols = df2.columns\n",
    "    merged_df[sentiment_cols] = merged_df[sentiment_cols].fillna(0)\n",
    "    return merged_df\n",
    "\n",
    "def keep_news(df):\n",
    "    news_df = df[df['ticker_sentiment'] != 0.0]\n",
    "    return news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1eba12f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_hourly = add_ta_features(btc_1h, 1)\n",
    "btc_4hourly = add_ta_features(btc_4h, 1)\n",
    "btc_daily = add_ta_features(btc_1d, 1)\n",
    "btc_weekly = add_ta_features(btc_1w, 1)\n",
    "\n",
    "btc_news_hourly = aggregate_and_forwardfill_news(btc_news, '1h')\n",
    "btc_news_4hourly = aggregate_and_forwardfill_news(btc_news, '4h')\n",
    "btc_news_daily = aggregate_and_forwardfill_news(btc_news, '1d')\n",
    "btc_news_weekly = aggregate_and_forwardfill_news(btc_news, '1W')\n",
    "\n",
    "eth_hourly = add_ta_features(eth_1h, 1)\n",
    "eth_4hourly = add_ta_features(eth_4h, 1)\n",
    "eth_daily = add_ta_features(eth_1d, 1)\n",
    "eth_weekly = add_ta_features(eth_1w, 1)\n",
    "\n",
    "eth_news_hourly = aggregate_and_forwardfill_news(eth_news, '1h')\n",
    "eth_news_4hourly = aggregate_and_forwardfill_news(eth_news, '4h')\n",
    "eth_news_daily = aggregate_and_forwardfill_news(eth_news, '1d')\n",
    "eth_news_weekly = aggregate_and_forwardfill_news(eth_news, '1W')\n",
    "\n",
    "aapl_hourly = add_ta_features(aapl_1h, 1)\n",
    "aapl_4hourly = add_ta_features(aapl_4h, 1)\n",
    "aapl_daily = add_ta_features(aapl_1d, 1)\n",
    "aapl_weekly = add_ta_features(aapl_1w, 1)\n",
    "\n",
    "aapl_news_hourly = aggregate_and_forwardfill_news(aapl_news, '1h')\n",
    "aapl_news_4hourly = aggregate_and_forwardfill_news(aapl_news, '4h')\n",
    "aapl_news_daily = aggregate_and_forwardfill_news(aapl_news, '1d')\n",
    "aapl_news_weekly = aggregate_and_forwardfill_news(aapl_news, 'W-FRI')\n",
    "\n",
    "tsla_hourly = add_ta_features(tsla_1h, 1)\n",
    "tsla_4hourly = add_ta_features(tsla_4h, 1)\n",
    "tsla_daily = add_ta_features(tsla_1d, 1)\n",
    "tsla_weekly = add_ta_features(tsla_1w, 1)\n",
    "\n",
    "tsla_news_hourly = aggregate_and_forwardfill_news(tsla_news, '1h')\n",
    "tsla_news_4hourly = aggregate_and_forwardfill_news(tsla_news, '4h')\n",
    "tsla_news_daily = aggregate_and_forwardfill_news(tsla_news, '1d')\n",
    "tsla_news_weekly = aggregate_and_forwardfill_news(tsla_news, 'W-FRI')\n",
    "\n",
    "amzn_hourly = add_ta_features(amzn_1h, 1)\n",
    "amzn_4hourly = add_ta_features(amzn_4h, 1)\n",
    "amzn_daily = add_ta_features(amzn_1d, 1)\n",
    "amzn_weekly = add_ta_features(amzn_1w, 1)\n",
    "\n",
    "amzn_news_hourly = aggregate_and_forwardfill_news(amzn_news, '1h')\n",
    "amzn_news_4hourly = aggregate_and_forwardfill_news(amzn_news, '4h')\n",
    "amzn_news_daily = aggregate_and_forwardfill_news(amzn_news, '1d')\n",
    "amzn_news_weekly = aggregate_and_forwardfill_news(amzn_news, 'W-FRI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b2ac879",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_btc_hourly = merge_df(btc_hourly, btc_news_hourly)\n",
    "merged_btc_4hourly = merge_df(btc_4hourly, btc_news_4hourly)\n",
    "merged_btc_daily = merge_df(btc_daily, btc_news_daily)\n",
    "merged_btc_weekly = merge_df(btc_weekly, btc_news_weekly)\n",
    "\n",
    "merged_eth_hourly = merge_df(eth_hourly, eth_news_hourly)\n",
    "merged_eth_4hourly = merge_df(eth_4hourly, eth_news_4hourly)\n",
    "merged_eth_daily = merge_df(eth_daily, eth_news_daily)\n",
    "merged_eth_weekly = merge_df(eth_weekly, eth_news_weekly)\n",
    "\n",
    "merged_aapl_hourly = merge_df(aapl_hourly, aapl_news_hourly)\n",
    "merged_aapl_4hourly = merge_df(aapl_4hourly, aapl_news_4hourly)\n",
    "merged_aapl_daily = merge_df(aapl_daily, aapl_news_daily)\n",
    "merged_aapl_weekly = merge_df(aapl_weekly, aapl_news_weekly)\n",
    "\n",
    "merged_tsla_hourly = merge_df(tsla_hourly, tsla_news_hourly)\n",
    "merged_tsla_4hourly = merge_df(tsla_4hourly, tsla_news_4hourly)\n",
    "merged_tsla_daily = merge_df(tsla_daily, tsla_news_daily)\n",
    "merged_tsla_weekly = merge_df(tsla_weekly, tsla_news_weekly)\n",
    "\n",
    "merged_amzn_hourly = merge_df(amzn_hourly, amzn_news_hourly)\n",
    "merged_amzn_4hourly = merge_df(amzn_4hourly, amzn_news_4hourly)\n",
    "merged_amzn_daily = merge_df(amzn_daily, amzn_news_daily)\n",
    "merged_amzn_weekly = merge_df(amzn_weekly, amzn_news_weekly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d143570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving for BTC\n",
    "merged_btc_hourly.to_csv(\"app/datasets/btc_hourly.csv\")\n",
    "merged_btc_4hourly.to_csv(\"app/datasets/btc_4hourly.csv\")\n",
    "merged_btc_daily.to_csv(\"app/datasets/btc_daily.csv\")\n",
    "merged_btc_weekly.to_csv(\"app/datasets/btc_weekly.csv\")\n",
    "\n",
    "# Saving for ETH\n",
    "merged_eth_hourly.to_csv(\"app/datasets/eth_hourly.csv\")\n",
    "merged_eth_4hourly.to_csv(\"app/datasets/eth_4hourly.csv\")\n",
    "merged_eth_daily.to_csv(\"app/datasets/eth_daily.csv\")\n",
    "merged_eth_weekly.to_csv(\"app/datasets/eth_weekly.csv\")\n",
    "\n",
    "# Saving for AAPL\n",
    "merged_aapl_hourly.to_csv(\"app/datasets/aapl_hourly.csv\")\n",
    "merged_aapl_4hourly.to_csv(\"app/datasets/aapl_4hourly.csv\")\n",
    "merged_aapl_daily.to_csv(\"app/datasets/aapl_daily.csv\")\n",
    "merged_aapl_weekly.to_csv(\"app/datasets/aapl_weekly.csv\")\n",
    "\n",
    "# Saving for TSLA\n",
    "merged_tsla_hourly.to_csv(\"app/datasets/tsla_hourly.csv\")\n",
    "merged_tsla_4hourly.to_csv(\"app/datasets/tsla_4hourly.csv\")\n",
    "merged_tsla_daily.to_csv(\"app/datasets/tsla_daily.csv\")\n",
    "merged_tsla_weekly.to_csv(\"app/datasets/tsla_weekly.csv\")\n",
    "\n",
    "# Saving for AMZN\n",
    "merged_amzn_hourly.to_csv(\"app/datasets/amzn_hourly.csv\")\n",
    "merged_amzn_4hourly.to_csv(\"app/datasets/amzn_4hourly.csv\")\n",
    "merged_amzn_daily.to_csv(\"app/datasets/amzn_daily.csv\")\n",
    "merged_amzn_weekly.to_csv(\"app/datasets/amzn_weekly.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
